{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages\n",
    "\n",
    "Tensforflow includes a data provider for MNIST.\n",
    "\n",
    "Ensure that the current environment has tensorflow-datasets module installed. \n",
    "\n",
    "For every usage on the dataset, it is stored in the respective folder (C:\\Users...\\tensorflow_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre Processing\n",
    "\n",
    "The tensorflow dataset is stored in the respective folder (C:\\Users...\\tensorflow_datasets).\n",
    "\n",
    "To get the MNIST dataset, use tfds.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) \n",
    "# apply MNIST as the argument for the name\n",
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "\n",
    "# with_info=True will provide a tuple containing information about the version, features, number of samples\n",
    "# use this information below and store in this\n",
    "\n",
    "\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target)\n",
    "# this will remain as such for the whole dataset that we refer to (in both train and test data)\n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# perfer to have inputs and targets separated\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>, 'test': <PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}\n"
     ]
    }
   ],
   "source": [
    "# Here, it is observed the shape of the tensor compromises 28 x 28 matrix, with 2-D dimension\n",
    "# The datatype is int.64 (or integer)\n",
    "print(mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='C:\\\\Users\\\\QS\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling this will show the info relating to the dataset\n",
    "# Do note that it has also two splits within the dataset ('test', 'train') and the corresponding number of samples attached to it\n",
    "\n",
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# once loaded the dataset, extract the training and testing dataset with the built references\n",
    "# by default, TF has training and testing datasets, but no validation sets\n",
    "\n",
    "mnist_train = mnist_dataset['train']\n",
    "mnist_test = mnist_dataset['test']\n",
    "\n",
    "# to create validation set, extract samples from training dataset since it contains a much larger sample (60000)\n",
    "# defining the number of validation samples as a % of the train samples\n",
    "# make use of mnist info to get the splits instead of counting the observations by get into the dictionary list under splits\n",
    "# the num_examples represents the total number of images that 'train' or 'test' contains\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "\n",
    "# make sure to cast the validations samples to an integer to avoid float error\n",
    "\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# defining training_samples is not necessary at this point as it will further define once pre process the data further (scale and shuffle)\n",
    "\n",
    "\n",
    "# similarly, assign test variable instead of using the mnisft_info variable\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "\n",
    "# make sure to cast the testing samples into an integer, like above\n",
    "\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.Tensor(6000, shape=(), dtype=int64)\n",
      "\n",
      " tf.Tensor(10000, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# to confirm that validation sample is a% of train dataset (60000 -> 6000)\n",
    "# Notice that the tensor rank is 0, meaning it only contains a scalar which in this is is 6,000\n",
    "print('',num_validation_samples) \n",
    "\n",
    "# assingment on testing sample\n",
    "# same as above, where the tensor rank is 0\n",
    "print('\\n', num_test_samples)\n",
    "\n",
    "# remember that we need to take this element having scalar 6000 and 10000 to their individual dataset (train or test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scale the data in a way that can be interpreted numerically\n",
    "# at the moment, the data is stored from  0 to 255 (0 represents purely black and 255 purely white)\n",
    "# we know that we want our response to either be 0 to 1 (hence the classification solution)\n",
    "\n",
    "def scale(image, label):\n",
    "    # cast the mnist_info.image from mnist_info\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # since the possible values for the inputs are 0 to 255 (256 different shades of grey)\n",
    "    # if we divide each element by 255, we would get the desired result -> all elements will be between 0 and 1\n",
    "    # the dot after 255 signifies that the return shall be float instead of numeric\n",
    "    image /= 255.\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the method .map() allows a custom transformation to be applied to a given dataset\n",
    "# scale both validation and training data from mnist_train\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "# next, scale and batch the test data\n",
    "# scale it so it has the same magnitude as the train and validation\n",
    "# there would be a single batch, equal to the size of the test data\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shuffling\n",
    "# once we have scaled our data, we need to shuffle the data to obtain a less biased estimatation of a true gradient\n",
    "# we need to set our buffer size to be 1<n<num_of_samples (70,000)\n",
    "\n",
    "BUFFER_SIZE = 10000 # hyper-parameter\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# next, we need to take all of validation sets that was created from the training samples and apply the shuffled to form a new dataset for validation\n",
    "# now this validation data would have contained shuffled dataset and scaled dataset\n",
    "# it is known taht validation_data should contain\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# repeat the same for training data, where this time we shall skip as this means that it skips the first 6,000 images that was reserved for validation set\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tf.Tensor(6000, shape=(), dtype=int64)\n",
      "\n",
      " <TakeDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
      "\n",
      " <SkipDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# notice that earlier, num_validation_samples contains only scalar\n",
    "print('',num_validation_samples)\n",
    "\n",
    "# applying the suffled and scaled data to take the samples from num_validation_samples will give 2 tensor elements, of which contains the 6,000 images dataset\n",
    "print('\\n',validation_data)\n",
    "\n",
    "# likewise, it will also treat the same to train_data\n",
    "print('\\n',train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batching\n",
    "# one of the best method for batching is to use mini batches gradient descent\n",
    "# we need to set our batch size to be 1<n<num_of_samples (70,000)\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "# batch train data\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "# as for validation, it is generally known that there would not be backpropagating (no update rule, hence no optimization)\n",
    "# therefore there will be no batch for validation\n",
    "# however, the model expects the validation data in the set of batch form too. In this case, the batch number = num_of_samples -> single batch\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# as for test data...\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
      "\n",
      " <BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
      "\n",
      " <BatchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# printing the batch dataset will give \"BatchDataset\" to show that it has batches\n",
    "print('',train_data)\n",
    "\n",
    "# same goes to validation data, but only contains a single batch\n",
    "print('\\n',validation_data)\n",
    "\n",
    "# test data will show it has batches\n",
    "print('\\n',test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# takes next batch (it is the only batch) and maintain that the validation_data is iterable\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "\n",
    "# iter(validation_data) makes the 'validation_data' object iterable. \n",
    "# This means it could be used like a loop. Imagine it has the values 1,12,-4,9 and that 1 is 'loaded'.\n",
    "# Using next(), it will load the next batch. Then the value of the object would be 12 as 1 has already been loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model\n",
    "\n",
    "When thinking about a deep learning algorithm, think about building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_size = 784 # input size\n",
    "output_size = 10 # output size\n",
    "hidden_layer_size = 200 # hidden layer size\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # CNNs (convulotional neural network) is unknown and no feed such input into our net, therefore need to flatten 28x28x1 images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows to create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    # for each hidden layer, need to specify this as a new argument under keras.Sequential\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='tanh'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# as for the learning rate, set new variable (optional), if not\n",
    "n = 0.001\n",
    "\n",
    "# create the Adam optimizer with the desired learning rate\n",
    "# the loss function is sparse categorical crossentropy\n",
    "# and the metrics for each iteration\n",
    "\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate = n)\n",
    "\n",
    "model.compile(optimizer=adam_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "270/270 - 2s - loss: 0.3480 - accuracy: 0.8987 - val_loss: 0.2086 - val_accuracy: 0.9413 - 2s/epoch - 8ms/step\n",
      "Epoch 2/10\n",
      "270/270 - 1s - loss: 0.1704 - accuracy: 0.9501 - val_loss: 0.1500 - val_accuracy: 0.9585 - 1s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "270/270 - 1s - loss: 0.1207 - accuracy: 0.9644 - val_loss: 0.1110 - val_accuracy: 0.9700 - 1s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "270/270 - 1s - loss: 0.0901 - accuracy: 0.9734 - val_loss: 0.0910 - val_accuracy: 0.9722 - 1s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "270/270 - 1s - loss: 0.0704 - accuracy: 0.9788 - val_loss: 0.0734 - val_accuracy: 0.9782 - 1s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "270/270 - 1s - loss: 0.0564 - accuracy: 0.9832 - val_loss: 0.0642 - val_accuracy: 0.9830 - 1s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "270/270 - 1s - loss: 0.0461 - accuracy: 0.9863 - val_loss: 0.0582 - val_accuracy: 0.9827 - 1s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "270/270 - 1s - loss: 0.0358 - accuracy: 0.9893 - val_loss: 0.0499 - val_accuracy: 0.9862 - 1s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "270/270 - 1s - loss: 0.0298 - accuracy: 0.9913 - val_loss: 0.0408 - val_accuracy: 0.9890 - 1s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "270/270 - 1s - loss: 0.0240 - accuracy: 0.9934 - val_loss: 0.0323 - val_accuracy: 0.9918 - 1s/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2879fbeab30>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the maximum number of epochs\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# fit the model specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data in the format: (inputs,targets)\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "After training on the training data and validating on the validation data, test the final prediction power of model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset.\n",
    "\n",
    "The test is the absolute final instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0673 - accuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07. Test accuracy: 97.96%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real world application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '8.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[210], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image \n\u001b[0;32m      2\u001b[0m image_path\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mimg_to_array(im)\n\u001b[0;32m      5\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)      \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py3-TF2.0\\lib\\site-packages\\keras\\utils\\image_utils.py:422\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[0;32m    421\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    423\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '8.png'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image \n",
    "image_path= \"8.png\"\n",
    "im = image.load_img(image_path, target_size=(28, 28), color_mode = \"grayscale\")\n",
    "img = image.img_to_array(im)\n",
    "img = tf.expand_dims(img, axis=0)      \n",
    "img /= 255\n",
    "model.predict_classes(img)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
